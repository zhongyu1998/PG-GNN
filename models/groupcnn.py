import copy

import torch
import torch.nn as nn
import torch.nn.functional as F


class GroupCNN(nn.Module):
    def __init__(self, num_layers, num_mlp_layers, input_dim, hidden_dim, output_dim,
                 final_dropout, graph_pooling_type, neighbor_pooling_type, device):
        """
            num_layers: number of layers in GNN (INCLUDING the input layer)
            num_mlp_layers: number of layers in MLP (EXCLUDING the input layer)
            input_dim: dimensionality of input features
            hidden_dim: dimensionality of hidden units in ALL layers
            output_dim: number of classes for prediction
            final_dropout: dropout ratio after the final linear prediction layer
            graph_pooling_type: how to aggregate all nodes in a graph (sum, average, or none)
            neighbor_pooling_type: how to aggregate neighboring nodes (sum, average, max, srn, gru, or lstm)
            device: which device to use
        """

        super(GroupCNN, self).__init__()

        self.num_layers = num_layers
        self.final_dropout = final_dropout
        self.graph_pooling_type = graph_pooling_type
        self.neighbor_pooling_type = neighbor_pooling_type
        self.device = device

        # a list of batch norms applied to the output of RNN (input of the final linear prediction layer)
        self.batch_norms = torch.nn.ModuleList()

        # a list of SRNs/GRUs/LSTMs
        self.rnns = torch.nn.ModuleList()

        for layer in range(self.num_layers-1):
            if layer == 0:
                if self.neighbor_pooling_type == "srn":
                    self.rnns.append(nn.RNN(input_dim, hidden_dim, num_layers=num_mlp_layers, batch_first=True))
                elif self.neighbor_pooling_type == "gru":
                    self.rnns.append(nn.GRU(input_dim, hidden_dim, num_layers=num_mlp_layers, batch_first=True))
                elif self.neighbor_pooling_type == "lstm":
                    self.rnns.append(nn.LSTM(input_dim, hidden_dim, num_layers=num_mlp_layers, batch_first=True))
                else:
                    pass
            else:
                if self.neighbor_pooling_type == "srn":
                    self.rnns.append(nn.RNN(hidden_dim, hidden_dim, num_layers=num_mlp_layers, batch_first=True))
                elif self.neighbor_pooling_type == "gru":
                    self.rnns.append(nn.GRU(hidden_dim, hidden_dim, num_layers=num_mlp_layers, batch_first=True))
                elif self.neighbor_pooling_type == "lstm":
                    self.rnns.append(nn.LSTM(hidden_dim, hidden_dim, num_layers=num_mlp_layers, batch_first=True))
                else:
                    pass

            self.batch_norms.append(nn.BatchNorm1d(hidden_dim))

        # a linear function that maps the hidden representation at each layer into a prediction score
        self.linears_prediction = torch.nn.ModuleList()
        for layer in range(self.num_layers):
            if layer == 0:
                self.linears_prediction.append(nn.Linear(input_dim, output_dim))
            else:
                self.linears_prediction.append(nn.Linear(hidden_dim, output_dim))

    def perm_group(self, result):
        n = len(result)
        last = copy.deepcopy(result)
        result_list = [copy.deepcopy(result)]

        if n % 2:
            for _ in range(n - 2):
                result[2] = last[1]
                result[4:n+1:2] = last[2:n-1:2]
                result[n - 2] = last[n - 1]
                result[1:n-2:2] = last[3:n:2]
                last = copy.deepcopy(result)
                result_list.append(copy.deepcopy(result))
        else:
            for _ in range(n - 1):
                result[1] = last[0]
                result[3:n+1:2] = last[1:n-1:2]
                result[n - 2] = last[n - 1]
                result[0:n-2:2] = last[2:n:2]
                last = copy.deepcopy(result)
                result_list.append(copy.deepcopy(result))

        return result_list

    def __preprocess_neighbors_grouppool(self, batch_graph):
        """
            create padded_neighbor_list and neighbor_num_list in concatenated (batch) graph
            padded_neighbor_list: a list of lists of lists, where the k-th sublist represents the list of group action
                                  results (permutation results) of sigma^k, and the k-th sublist contains sub-sublists
                                  with the form of [central node v, permutation generated by sigma^k, central node v, paddings with -1]
            neighbor_num_list:    a list of lists, where the k-th sublist contains real lengths (sequence lengths) of
                                  [central node v, permutation generated by sigma^k, central node v] for different v
            NOTE:                 since k <= n - 1 - n % 2, only when the central node v has n >= (k + 2 - k % 2) neighbors
                                  will it (its permutation group) possess sigma^k and appear in the k-th sublist
        """

        # compute the maximum number of neighbors in the current mini-batch
        max_deg = max([graph.max_neighbor for graph in batch_graph]) + 2  # +2: add the central node twice

        padded_neighbor_list = [[] for _ in range(max_deg - max_deg % 2 - 2)]
        neighbor_num_list = [[] for _ in range(max_deg - max_deg % 2 - 2)]
        start_idx = [0]

        for i, graph in enumerate(batch_graph):  # each graph i
            start_idx.append(start_idx[i] + len(graph.g))
            for j in range(len(graph.neighbors)):  # each node j in graph i
                # add an offset value to each neighbor index
                initial = [n + start_idx[i] for n in graph.neighbors[j]]
                # act the permutation group on neighboring nodes
                result_list = self.perm_group(initial)
                # after group actions, we deal with each permutation result k of node j's neighbors
                for k in range(len(result_list)):
                    pad = [j + start_idx[i]]  # central node
                    pad.extend(result_list[k])  # permutation k of neighbors
                    pad.append(j + start_idx[i])  # central node
                    pad.extend([-1] * (max_deg - len(pad)))  # padding, dummy data is stored in -1
                    padded_neighbor_list[k].append(pad)
                    neighbor_num_list[k].append(len(graph.neighbors[j]) + 2)

        return padded_neighbor_list, neighbor_num_list

    def __preprocess_graphpool(self, batch_graph):
        # create a sum or average pooling sparse matrix (num graphs x num nodes) for concatenated (batch) graph

        start_idx = [0]
        # compute the index of the starting node in each graph
        for i, graph in enumerate(batch_graph):
            start_idx.append(start_idx[i] + len(graph.g))

        idx = []
        elem = []
        for i, graph in enumerate(batch_graph):
            if self.graph_pooling_type == "average":
                # average pooling
                elem.extend([1./len(graph.g)] * len(graph.g))
            else:
                # sum pooling
                elem.extend([1] * len(graph.g))
            idx.extend([[i, j] for j in range(start_idx[i], start_idx[i+1], 1)])

        elem = torch.FloatTensor(elem)
        idx = torch.LongTensor(idx).transpose(0, 1)
        graph_pool = torch.sparse.FloatTensor(idx, elem, torch.Size([len(batch_graph), start_idx[-1]]))

        return graph_pool.to(self.device)

    def grouppool(self, h, layer, padded_neighbor_list, neighbor_num_list):
        dummy = torch.zeros_like(h[0])
        h_with_dummy = torch.cat([h, dummy.reshape((1, -1)).to(self.device)])
        pooled_rep = 0

        for k in range(len(padded_neighbor_list)):
            pad_sequence = h_with_dummy[torch.LongTensor(padded_neighbor_list[k])]
            pack_sequence = nn.utils.rnn.pack_padded_sequence(pad_sequence, torch.LongTensor(neighbor_num_list[k]),
                                                              batch_first=True, enforce_sorted=False)
            if self.neighbor_pooling_type == "lstm":
                output, (h_n, c_n) = self.rnns[layer](pack_sequence)
            else:
                output, h_n = self.rnns[layer](pack_sequence)

            if k == 0:
                rep = h_n[-1, :, :]
                pooled_rep = torch.zeros_like(rep)
            else:
                lower_bound = k + 2 - k % 2 + 2  # see line 99
                rep = torch.zeros_like(pooled_rep)
                rep[torch.LongTensor(neighbor_num_list[0]) >= lower_bound] = h_n[-1, :, :]

            pooled_rep += rep

        return pooled_rep

    def next_layer(self, h, layer, padded_neighbor_list=None, neighbor_num_list=None):
        # aggregate the central node and neighboring nodes altogether
        pooled_rep = self.grouppool(h, layer, padded_neighbor_list, neighbor_num_list)

        # batch normalization
        h = self.batch_norms[layer](pooled_rep)

        # non-linearity
        h = F.relu(h)

        return h

    def forward(self, batch_graph):
        X_concat = torch.cat([graph.node_features for graph in batch_graph], 0).to(self.device)
        graph_pool = self.__preprocess_graphpool(batch_graph)

        padded_neighbor_list, neighbor_num_list = self.__preprocess_neighbors_grouppool(batch_graph)

        # a list of hidden representations at each layer (including the input)
        hidden_rep = [X_concat]
        h = X_concat

        for layer in range(self.num_layers-1):
            h = self.next_layer(h, layer, padded_neighbor_list=padded_neighbor_list, neighbor_num_list=neighbor_num_list)
            hidden_rep.append(h)

        score_over_layer = 0

        # perform pooling for all nodes in each graph
        for layer, h in enumerate(hidden_rep):
            if self.graph_pooling_type == "none":
                pooled_h = h
            else:
                pooled_h = torch.spmm(graph_pool, h)
            score_over_layer += F.dropout(self.linears_prediction[layer](pooled_h), self.final_dropout, training=self.training)

        return score_over_layer
